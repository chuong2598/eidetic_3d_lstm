{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ed3_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "3hymnPNbIxEk",
        "outputId": "7f421e71-c170-486c-856d-ee5e49145578"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.cuda.get_device_name(0)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ1zghkmJK8s"
      },
      "source": [
        "\n",
        "class ED3LSTMCell(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, input_shape, in_channels, hidden_channels, kernel_size, stride):\n",
        "        super(ED3LSTMCell, self).__init__()\n",
        "        \n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.padding = kernel_size//2\n",
        "        self.stride = stride\n",
        "        \n",
        "        self.r_bias, self.i_bias, self.g_bias, self.i_prime_bias, self.g_prime_bias, self.f_prime_bias, self.o_bias = torch.nn.Parameter(torch.randn(7))\n",
        "        \n",
        "        channel, length, width, height = input_shape\n",
        "\n",
        "        self.conv_x = torch.nn.Sequential(\n",
        "            torch.nn.Conv3d(in_channels=in_channels, out_channels=hidden_channels*7, kernel_size=kernel_size, padding=self.padding, stride=stride),\n",
        "            torch.nn.LayerNorm([hidden_channels*7, length, width, height])\n",
        "        )\n",
        "        \n",
        "        self.conv_h_prev = torch.nn.Sequential(\n",
        "            torch.nn.Conv3d(in_channels=hidden_channels, out_channels=hidden_channels*4, kernel_size=kernel_size, padding=self.padding, stride=stride),\n",
        "            torch.nn.LayerNorm([hidden_channels*4, length, width, height])\n",
        "        )\n",
        "        \n",
        "        self.conv_m_prev = torch.nn.Sequential(\n",
        "            torch.nn.Conv3d(in_channels=hidden_channels, out_channels=hidden_channels*3, kernel_size=kernel_size, padding=self.padding, stride=stride),\n",
        "            torch.nn.LayerNorm([hidden_channels*3, length, width, height])\n",
        "        )\n",
        "        \n",
        "        self.conv_c = torch.nn.Sequential(\n",
        "            torch.nn.Conv3d(in_channels=hidden_channels, out_channels=hidden_channels, kernel_size=kernel_size, padding=self.padding, stride=stride),\n",
        "            torch.nn.LayerNorm([hidden_channels, length, width, height])\n",
        "        )\n",
        "        \n",
        "        self.conv_m = torch.nn.Sequential(\n",
        "            torch.nn.Conv3d(in_channels=hidden_channels, out_channels=hidden_channels, kernel_size=kernel_size, padding=self.padding, stride=stride),\n",
        "            torch.nn.LayerNorm([hidden_channels, length, width, height])\n",
        "        )\n",
        "        \n",
        "        self.layer_norm_c = torch.nn.LayerNorm([hidden_channels, length, width, height])\n",
        "        \n",
        "        self.conv_c_m = torch.nn.Conv3d(in_channels=hidden_channels*2, out_channels=hidden_channels, kernel_size=1, padding=0, stride=1)\n",
        "        \n",
        "    def forward(self, x, h_prev, c_history, m_prev, tau=-1):\n",
        "        batch, channels, length, height, width = x.shape\n",
        "        c_prev = c_history[-1]\n",
        "        if (tau==-1):\n",
        "            tau = len(c_history)\n",
        "        c_history = torch.stack(c_history[-tau:])\n",
        "        c_history = c_history.permute(1,0,2,3,4,5)\n",
        "#         c_history = torch.cat([c_history[-i].reshape(batch, 1, self.hidden_channels, length, height, width) for i in range(tau, 0, -1)], dim=1)\n",
        "#         print(c_history.shape)\n",
        "        \n",
        "        conv_x = self.conv_x(x)\n",
        "        r_x, i_x, g_x, i_x_prime, g_x_prime, f_x_prime, o_x = torch.split(tensor=conv_x, split_size_or_sections=self.hidden_channels, dim=1)\n",
        "        conv_h_prev = self.conv_h_prev(h_prev)\n",
        "        r_h, i_h, g_h, o_h = torch.split(tensor=conv_h_prev, split_size_or_sections=self.hidden_channels, dim=1)\n",
        "        \n",
        "        r = torch.sigmoid(r_x + r_h + self.r_bias)\n",
        "        i = torch.sigmoid(i_x + i_h + self.i_bias)\n",
        "        g = torch.tanh(g_x + g_h + self.g_bias)\n",
        "        \n",
        "        # Self attention with query=R and key=value= historical memories c (c_history)\n",
        "        r = r.reshape(batch, length*height*width, self.hidden_channels)\n",
        "        c_history = c_history.reshape(batch, tau*length*height*width, self.hidden_channels)\n",
        "\n",
        "        # !!!!!!!!!!!!!!! Error here: Tesla V100-SXM2-16GB does not have enough memory to perform softmax !!!!!!!!!!!!!!!\n",
        "        recall = torch.softmax(r @ c_history.permute(0, 2, 1), dim=1) @ c_history \n",
        "        recall = recall.reshape(i.shape)\n",
        "        \n",
        "        c_new = (i * g) + self.layer_norm_c(c_prev + recall)\n",
        "        \n",
        "        conv_m_prev = self.conv_m_prev(m_prev)\n",
        "        i_m_prime, g_m_prime, f_m_prime = torch.split(tensor=conv_m_prev, split_size_or_sections=self.hidden_channels, dim=1)\n",
        "        \n",
        "        i_prime = torch.sigmoid(i_x_prime + i_m_prime + self.i_prime_bias)\n",
        "        g_prime = torch.tanh(g_x_prime + g_m_prime + self.g_prime_bias)\n",
        "        f_prime = torch.sigmoid(f_x_prime + f_m_prime + self.f_prime_bias)\n",
        "        m_new = i_prime*g_prime + f_prime*m_prev\n",
        "        \n",
        "        o_c = self.conv_c(c_new)\n",
        "        o_m = self.conv_m(m_new)\n",
        "        \n",
        "        o = torch.sigmoid(o_x + o_h + o_c + o_m + self.o_bias)\n",
        "        c_m_cat = torch.cat((c_new, m_new), dim = 1)\n",
        "        h_new = o * torch.tanh(self.conv_c_m(c_m_cat))\n",
        "        \n",
        "        return h_new, c_new, m_new\n",
        "        \n",
        "        "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DBly7LhJMux"
      },
      "source": [
        "\n",
        "class ED3LSTM(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, nb_layers, encoder_hidden_layer_dim, input_shape, in_channel, hidden_layer_dim, kernel_size, stride):\n",
        "        super(ED3LSTM, self).__init__()\n",
        "        \n",
        "        self.nb_layers = nb_layers\n",
        "        self.hidden_layer_dim = hidden_layer_dim\n",
        "        \n",
        "        channels, length, height, width = input_shape\n",
        "        \n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            torch.nn.Conv3d(in_channels=channels, out_channels=encoder_hidden_layer_dim, kernel_size=kernel_size, padding=kernel_size//2, stride=1)\n",
        "        )\n",
        "        \n",
        "        ed3_lstm_cells = []\n",
        "        for i in range(nb_layers):\n",
        "            if i == 0:\n",
        "                new_cell = ED3LSTMCell(input_shape=input_shape, in_channels=encoder_hidden_layer_dim, hidden_channels=hidden_layer_dim, kernel_size=kernel_size, stride=stride)\n",
        "            else:\n",
        "                new_cell = ED3LSTMCell(input_shape=input_shape, in_channels=hidden_layer_dim, hidden_channels=hidden_layer_dim, kernel_size=kernel_size, stride=stride)\n",
        "            ed3_lstm_cells.append(new_cell)\n",
        "            \n",
        "        self.ed3_lstm_cells = torch.nn.ModuleList(ed3_lstm_cells)\n",
        "\n",
        "        \n",
        "        \n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Conv3d(in_channels=hidden_layer_dim, out_channels=channels, kernel_size=(window_size, 1, 1), padding=0, stride=(window_size, 1, 1))\n",
        "        )\n",
        "\n",
        "    def forward(self, input_sequence, device=\"cuda\"):\n",
        "        \"\"\"batch_sequence: tensor with shape (batch, channel, nb_element, window_size, height, width) containing batch of slided consecutive frames.\"\"\"\n",
        "        batch, channel, nb_element, window_size, height, width = input_sequence.shape\n",
        "        \n",
        "        \n",
        "        # 1-D list with length nb_layers including h of each layers\n",
        "        h_list = []\n",
        "        # 2-D list. The first dimension represent list of cell memory of a specific layer.\n",
        "        # Length of first dimention is nb_layers. Length of second dimension is nb_element (i.e number of timesteps)\n",
        "        # The element in the second dimension is a tensor representating a cell memeory at a specific layer and a specific timestep\n",
        "        c_list = []\n",
        "        # Store list of prediction\n",
        "        prediction = []\n",
        "        for layer in range(self.nb_layers):\n",
        "            h_list.append(torch.zeros(batch, self.hidden_layer_dim, window_size, height, width, device=device))\n",
        "            c_list.append([])\n",
        "            for time_step in range(nb_element):\n",
        "                c_list[layer].append(torch.zeros(batch, self.hidden_layer_dim, window_size, height, width, device=device))\n",
        "        \n",
        "        memory = torch.zeros(batch, self.hidden_layer_dim, window_size, height, width, device=device)\n",
        "        for time_step in range(nb_element):\n",
        "            \n",
        "            encoder_output = self.encoder(input_sequence[:,:,time_step])\n",
        "            c_history = c_list[0][:time_step+1]\n",
        "            h_list[0], c_list[0][time_step], memory = self.ed3_lstm_cells[0](encoder_output, h_list[0], c_history, memory)\n",
        "            \n",
        "            for layer in range(1, self.nb_layers):\n",
        "                c_history = c_list[layer][:time_step+1]\n",
        "                h_list[layer], c_list[layer][time_step], memory = self.ed3_lstm_cells[layer](h_list[layer-1], h_list[layer], c_history, memory)\n",
        "            timestep_prediction = self.decoder(h_list[-1])\n",
        "            prediction.append(timestep_prediction)\n",
        "        prediction = torch.stack(prediction).squeeze().permute(1,2,0,3,4)\n",
        "        return prediction\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM9Lbk4hI4Sn"
      },
      "source": [
        "\n",
        "def extract_slided_sequence(batch_sequences, window_size=2, window_stride=1):\n",
        "    batch, channels, length, height, width  = batch_sequences.shape\n",
        "    nb_elements = (length - window_size) // window_stride + 1\n",
        "    slided_sequences = torch.zeros(batch, channels, nb_elements, window_size, height, width)\n",
        "    \n",
        "    for i in range(0, length - window_size+1, window_stride):\n",
        "        element = batch_sequences[:, :, i:i+window_size, ...]\n",
        "        slided_sequences[:,:,i//window_stride,:,:,:] = element\n",
        "    return slided_sequences\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIyyrI9JTtxy"
      },
      "source": [
        "# from pynvml import *\n",
        "# nvmlInit()\n",
        "# h = nvmlDeviceGetHandleByIndex(0)\n",
        "# info = nvmlDeviceGetMemoryInfo(h)\n",
        "# print(f'total    : {info.total}')\n",
        "# print(f'free     : {info.free}')\n",
        "# print(f'used     : {info.used}')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_NPz9WeJS_v"
      },
      "source": [
        "\n",
        "# # !!! X train of shape batch x channel x nb_element x window_size x height x width\n",
        "window_size = 2\n",
        "window_stride = 1\n",
        "\n",
        "# # Load dataset\n",
        "training_set =  np.load(\"./drive/MyDrive/Side project/pred_cnn/mnist_train_seq.npy\")\n",
        "training_set = training_set.reshape(80000, 20, 1, 64, 64)\n",
        "x_train, y_train = torch.tensor(training_set[:10000, :10, ...]).float(), torch.tensor(training_set[:10000, window_size:10+window_size, ...]).float()\n",
        "x_train = x_train.permute(0, 2, 1, 3, 4)\n",
        "y_train = y_train.permute(0, 2, 1, 3, 4)\n",
        "\n",
        "testing_set = np.load(\"./drive/MyDrive/Side project/pred_cnn/mnist_test_seq.npy\")\n",
        "testing_set = testing_set.reshape(10000, 20, 1, 64, 64)\n",
        "x_val, y_val = torch.tensor(testing_set[:5000, :10, ...]).float(), torch.tensor(testing_set[:5000, window_size:10+window_size, ...]).float()\n",
        "x_val = x_val.permute(0, 2, 1, 3, 4)\n",
        "y_val = y_val.permute(0, 2, 1, 3, 4)\n",
        "\n",
        "x_test, y_test = torch.tensor(testing_set[5000:, :10, ...]).float(), torch.tensor(testing_set[5000:, window_size:10+window_size, ...]).float()\n",
        "x_test = x_test.permute(0, 2, 1, 3, 4)\n",
        "y_test = y_test.permute(0, 2, 1, 3, 4)\n",
        "\n",
        "del testing_set\n",
        "del training_set\n",
        "\n",
        "# Extract training, validation, testing set\n",
        "x_train = extract_slided_sequence(x_train, window_size, window_stride)\n",
        "x_val = extract_slided_sequence(x_val, window_size, window_stride)\n",
        "x_test = extract_slided_sequence(x_test, window_size, window_stride)\n",
        "\n",
        "training_set = DataLoader(TensorDataset(x_train, y_train), batch_size=1, shuffle=True)\n",
        "validation_set = DataLoader(TensorDataset(x_val, y_val), batch_size=1, shuffle=False)\n",
        "testing_set = DataLoader(TensorDataset(x_test, y_test), batch_size=1, shuffle=False)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x50vPUqcJaKX"
      },
      "source": [
        "\n",
        "channels = 1\n",
        "height = 64\n",
        "width = 64\n",
        "nb_layers = 4\n",
        "encoder_hidden_layer_dim = 64\n",
        "hidden_layer_dim = 64\n",
        "kernel_size = 5\n",
        "stride = 1\n",
        "\n",
        "# Init model, parameters\n",
        "ed3_lstm = ED3LSTM(nb_layers=nb_layers, encoder_hidden_layer_dim=encoder_hidden_layer_dim, input_shape=(channels, window_size, height, width), \n",
        "                   in_channel=channels, hidden_layer_dim=hidden_layer_dim, kernel_size=kernel_size, stride=stride)\n",
        "ed3_lstm.to(device=device)\n",
        "optim = torch.optim.Adam(ed3_lstm.parameters())\n",
        "l1_loss = torch.nn.L1Loss()\n",
        "l2_loss = torch.nn.MSELoss()\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "current_epoch = 0\n",
        "epochs = 20\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfA27TwAJw8N"
      },
      "source": [
        "\n",
        "\n",
        "for sequence, target in tqdm(training_set):\n",
        "    sequence = sequence.to(device=device)\n",
        "    target = target.to(device=device)\n",
        "    pred = ed3_lstm(sequence)\n",
        "    loss = l2_loss(pred, target) + l1_loss(pred, target)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71oVMV7WN_ZZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN-KGB04OB-z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}